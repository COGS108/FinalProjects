{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks for Protein Secondary Structure Prediction\n",
    "\n",
    "- David Wang\n",
    "- Michelle Franc Ragsac\n",
    "- Jimmy Quach\n",
    "- Dhaivath Raghupathy\n",
    "- Shih-Cheng Huang "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is it important to predict secondary structures?\n",
    "\n",
    "Within the body, proteins can be found in various shapes and sizes, and each possessing their own function. Some can be structural components that help retain an organ’s rigidity, and others can be metabolic components, breaking apart or putting together biomolecules. All proteins have several layers of structure that are important to the process of protein folding and their underlying functions. The most basic structure is the sequence of amino acid residues themselves. Amino acids are organic compounds that contain amine and carboxyl chemical groups, and are distinguishable from one another from their side chain, a connected chemical group. This side chain gives amino acids different chemical properties, such as differences in hydrophobicity, electrical charge, and branching groups. \n",
    "\n",
    "The secondary structure of proteins, which we will focus on during this project, refers to local folded structures that form due to interactions in the protein “backbone”. Due to their side chains, certain amino acids might be more or less inclined to conform to one of these secondary conformations and will thus ultimately affect their resulting function. There are 8 total types of secondary structures indicated by the Dictionary of Protein Secondary Structure, each with their specific functions and properties. These include:\n",
    "\n",
    "- The 3,10 Helix which contains a minimum length of 3 residues, \n",
    "- The Alpha Helix which is a minimum length of 4 residues, \n",
    "- The Pi Helix which contains a minimum length of 5 residues, \n",
    "- The T structure which has 3 - 5 turns of hydrogen bonding, \n",
    "- The E structure which is a minimum of 2 residues and extends parallel or anti-parallel to the Beta sheet, \n",
    "- The B structure which contains a residue within the beta bridge, \n",
    "- The S structure that has no hydrogen bonds, and finally \n",
    "- The C structure(s) which are structures that do not belong to any of the other categories. \n",
    "\n",
    "An example of of the effect of protein structures can be seen with human diseases like Sickle Cell Anemia which results from a defect in a protein’s three-dimensional structure due to properties from its secondary structure.\n",
    "\n",
    "For this project, we plan to answer: How can we determine protein structure through the sequencing data obtained from proteins? To tackle this question, we plan on applying machine learning techniques such as neural networks to develop a tool that uses sequencing data as input and returns a predicted structure. Furthermore using the resulting output we also wish to determine if there are any correlations between properties of proteins and the prediction accuracies that could further assist in predicting protein structure. With a massive amount of protein sequence data produced by sequencing technologies, the role of protein structure prediction is now more important than ever!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "We will be using a dataset from the ICML 2014 Deep Supervised and Convolutional Generative Stochastic Network paper for this study: <a href=http://www.princeton.edu/~jzthree/datasets/ICML2014/><strong>cullpdb+profile_6133</strong></a>. The data is originally in numpy format as a (N protein x k features) matrix, but we reshape this to (N protein x 700 amino acids x 44 features). Of these 44 features, 22 are amino acids, and the other 22 are a combination of relative and absolute solvent accessibility and sequence profiles. \n",
    "\n",
    "Solvent accessibility refers to the potential amount of water that can touch and react with the amino acids in the protein. This is very important because as mentioned above, amino acids possess various chemical properties, one of which includes their willingness to interact with water. Thus, the amino acids will conform into various structures that encourage their existing behaviors and properties. \n",
    "\n",
    "Sequence profiles are utilized as a feature because they assist in accounting for the genetic variability present within the sequences. Profiles account for varying probabilities of amino acids appearing in a certain position within a sequence and thus due to certain amino acids having particular functions and tendencies such as their willingness to interact with water, it allows us to account for possible variations of the sequence anThe data is originally in numpy format as a (N protein x k features) matrix, but we reshape this to (N protein x 700 aUnfmino acids x 44 features). Of these 44 features, 22 are amino acids, and the other 22 are a combination of relative and absolute solvent accessibility and sequence profiles. This will provide a stronger analysis to what structure is derived from the inputted sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "We used one-hot-encoding to represent the 21 different amino acids in our dataset, along with the secondary structure labels, which were also one-hot-encoded. The amino acid sequences were of various lengths, so we used a 22nd entry in the one-hot-encoded vector of amino acids to represent “no sequence”. We reshaped the inputs as described in the data description section to feed it into the network, and also split our data into three sets: a training, validation, and test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis & Results\n",
    "\n",
    "After cleaning and pre-processing our data, we fed this as the input to a neural network, which is built using TensorFlow. We have included the code segments for the neural network later in this section. \n",
    "\n",
    "The hyperparameters used in our network as follows:\n",
    "\n",
    "![hyperparameters table](Pr_023-master/hyperparameter_values_table.png \"Hyperparameters Table\")\n",
    "\n",
    "Here is an image of our network topology, followed by an in-depth explanation incase our audience is not familiar with neural nets:\n",
    "\n",
    "![network topology diagram](Pr_023-master/network_topology_diagram.png \"Network Topology Diagram\")\n",
    "\n",
    "We split our 44 features into two 22-feature categories: amino acids residues and relative/absolute solvent accessibility. We feed these into two separate 1D CNN’s as input. These CNN’s will use filter sizes of 3, 5, and 7 independently sliding through the sequences to look for amino acid motifs(repeated sequences). Each filter size has 16 different filters. We use same-padding, so each CNN output is a (700 x #Filters) tensor. Then we apply batch normalization as a form of regularization and to speed up training. We concatenate these 3 output tensors, along with the original input(to avoid losing information between layers) independently for both categories of features. \n",
    "\n",
    "We repeat the convolutional layer for both feature categories with filter sizes of 1, 2, and 3. This time, when concatenating the 3 output tensors, we include the output of the first CNN (of filter sizes 3, 5, and 7) instead of the original input. \n",
    "\n",
    "Now we combine the results of the two categories(vertical stack), reshape the tensor, and feed this into a fully connected layer. We take the output of this layer, and feed it as input to a bidirectional Recurrent Neural Network (RNN). Each direction is comprised of a 250 LSTM (Long Short Term Memory) cells. This is done to capture features based on relative position of amino acids in the sequence. \n",
    "\n",
    "We feed the output of the RNN to two more fully connected layers(The image above only shows 1, but this is due to lack of space on the page). The output of the final layer is (700 amino acids x 9 secondary structures) representing probability of each of the 9 structures based on the given sequence, where the 9 structures include the 8 structures mentioned above as well as an additional structure category for sequences that were predicted to have no structure. We use softmax with cross-entropy loss to predict the structure, and compare it with the true value. We trained the network for 1000 epochs(passes through the data), judging our improvement against the validation set at the end of each epoch.\n",
    "\n",
    "Finally, we examined our results on the test set and analyzed the results. \n",
    "The result of the output layer is a 700 x 9 tensor, representing the probability distribution(across the 9 secondary structures) for each amino acid. We take the maximum of the 9 probabilities for each of the 700 amino acids, resulting in our 700 x 1 prediction of secondary structures. To calculate the accuracy, we compare this vector with the input labels (across the 700 amino acids), and report the correct proportion. We originally observed a very high accuracy after training. We realized this is attributed to correct predictions of zero padding for sequences shorter than 700 amino acids. This zero padding does not contribute to the biological classification of secondary protein structure, so our accuracy measure only reports predictions for the valid amino acids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Data & Constructing the Neural Network\n",
    "\n",
    "Within this section, we have added the the code segments used in parsing the ICML 2014 dataset, as well as constructing the neural network. The files can also be found at: ```marshuang80/```<a href=\"https://github.com/marshuang80/Protein_structure_prediction\">```Protein_structure_prediction```.</a>\n",
    "\n",
    "<b>** Please note that these code segments were not directly run in this notebook because they would have taken hours to run. We ran these segments on a groupmate's computer. **</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```Protein_structure_prediction/read_data.py```\n",
    "\n",
    "Within this repository, the <b><a href=\"https://github.com/marshuang80/Protein_structure_prediction/blob/master/read_data.py\">```read_data.py```</a></b> file parses our data files and generates the neural network inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    '''Read the input file and split the data input features and labels\n",
    "    Args:\n",
    "        file_name (String): input file name\n",
    "    Return:\n",
    "        seq (np.array): lists of one-hot encoded amino acid sequence\n",
    "        f (np.array): list of properties for the protein sequence\n",
    "        y (np.array): one-hot encoded labels\n",
    "    '''\n",
    "    # Load input data\n",
    "    input_data = np.load(file_name)\n",
    "    input_data.shape = (6133,700,57)\n",
    "\n",
    "    # Get one-hot encoded labels\n",
    "    y = input_data[:,:,22:31].astype(np.float32)\n",
    "\n",
    "    # One-hot encoded amino acid seqeuences\n",
    "    seq = input_data[:,:,:22].astype(np.float32)\n",
    "\n",
    "    # Other features: N & C terminals, solvent accessibility, sequence profile\n",
    "    f = input_data[:,:,35:].astype(np.float32)\n",
    "\n",
    "    return seq,f,y\n",
    "\n",
    "\n",
    "def test_batch(x,f,y,split):\n",
    "    '''Split the input data into train and test batches\n",
    "    Args:\n",
    "        x (list): aa features\n",
    "        f (list): sequence profiles\n",
    "        y (list): sequence labels\n",
    "        ratio (float): ratio of training to testing set\n",
    "    Return:\n",
    "        x_train: batch of train aa\n",
    "        f_train: batch of trian features\n",
    "        y_train: batch of train label\n",
    "        x_test: batch of test aa\n",
    "        f_test: batch of test features\n",
    "        y_test: batch of test label\n",
    "    '''\n",
    "    x_train, f_train, y_train = x[:split], f[:split], y[:split]\n",
    "    x_test, f_test, y_test = x[split:], f[split:], y[split:]\n",
    "\n",
    "    return x_train, f_train, y_train, x_test, f_test, y_test\n",
    "\n",
    "\n",
    "def valid_batch(x,f,y,batch_size, ratio):\n",
    "    '''Generate random batches of samples during training\n",
    "    Args:\n",
    "        x (list): input aa\n",
    "        f (list): input features\n",
    "        y (list): sequence labels\n",
    "        batch_size (int): size of each training batch\n",
    "        ratio (float): ratio of training to validation set\n",
    "    Return:\n",
    "        x_train: batch of aa\n",
    "        f_train: batch of features\n",
    "        y_train: batch of train label\n",
    "        x_valid: batch of validation data\n",
    "        f_valid: batch of features\n",
    "        y_valid: batch of validation label\n",
    "     '''\n",
    "    # Sample data with batch_size\n",
    "    idx = random.sample(range(len(x)),batch_size)\n",
    "    xs, fs, ys = x[idx], f[idx], y[idx]\n",
    "\n",
    "\t# Generate batches\n",
    "    split = int(len(xs) * ratio)\n",
    "    x_train, f_train, y_train = xs[:split], fs[:split], ys[:split]\n",
    "    x_valid, f_valid, y_valid = xs[split:], fs[split:], ys[split:]\n",
    "    return x_train, f_train, y_train, x_valid, f_valid, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```Protein_structure_prediction/structure_prediction.py```\n",
    "\n",
    "The <b><a href=\"https://github.com/marshuang80/Protein_structure_prediction/blob/master/structure_prediction.py\">```structure_prediction.py```</a></b> file contains the neural network used to predict protein secondary structures. \n",
    "\n",
    "<i>Note: Within this final report, this block of code was not run within the notebook because it takes several hours to complete. We instead completed this segment outside of the notebook and then compiled the data afterwards.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# we have included read_data in a different cell\n",
    "# from read_data import *\n",
    "\n",
    "# Neural network parameters\n",
    "file_name = 'cullpdb+profile_6133.npy.gz'\n",
    "batch_size = 128\n",
    "epoches = 1000\n",
    "learning_rate = 0.0005\n",
    "num_input_variables = 22\n",
    "num_filter = 16\n",
    "num_rnn_cells = 256\n",
    "sequence_length = 700\n",
    "hidden_neurons_1 = 512\n",
    "hidden_neurons_2 = 256\n",
    "hidden_neurons_3= 256\n",
    "num_output_class = 9\n",
    "valid_ratio = 1\n",
    "filter_sizes = [3,5,7]\n",
    "filter_sizes_2 = [1,2,3]\n",
    "random_seed = 7\n",
    "\n",
    "# Lists to store outputs\n",
    "losses = []\n",
    "accs = []\n",
    "\n",
    "# Input placeholders\n",
    "x_input = tf.placeholder(np.float32, shape=[None, sequence_length, num_input_variables])\n",
    "var_input = tf.placeholder(np.float32, shape=[None, sequence_length, 22])\n",
    "y_label = tf.placeholder(np.float32, shape=[None, sequence_length, num_output_class])\n",
    "alpha = tf.placeholder(tf.float32, shape = [])\n",
    "\n",
    "\n",
    "def conv_aa_layers(x_input):\n",
    "    '''The function generates a few convolutional network layers with 3,5,7\n",
    "    filter lengths for the one-hot encoded amino acid sequence.\n",
    "    Args:\n",
    "        x_input (tf.placeholder): The batch of one hot encoded protein sequence\n",
    "    Returns:\n",
    "        conv_output (list): List of outputs from each CNN layer\n",
    "     '''\n",
    "    conv_outputs = []\n",
    "\n",
    "    # Using 1d convolution to scan over the sequence with window of \"size\"\n",
    "    for size in filter_sizes:\n",
    "\n",
    "        with tf.name_scope(\"conv_%i\"%(size)):\n",
    "            # Define convolution weights\n",
    "            conv_w = tf.get_variable(\"conv_%i\"%size, shape = [size, num_input_variables,\n",
    "               num_filter], initializer = tf.contrib.layers.xavier_initializer(seed = random_seed))\n",
    "\n",
    "            # Get convolution output from filters\n",
    "            conv_out_w = tf.nn.conv1d(x_input, conv_w, stride = 1,\n",
    "                        padding = \"SAME\",)\n",
    "\n",
    "            # Activation\n",
    "            conv_output = tf.nn.relu(conv_out_w, name = \"conv_%i_activation\"%size)\n",
    "\n",
    "            # Batch nomalization\n",
    "            conv_output = tf.layers.batch_normalization(conv_output)\n",
    "\n",
    "        conv_outputs.append(conv_output)\n",
    "\n",
    "    return conv_outputs\n",
    "\n",
    "\n",
    "def conv_features_layers(var_input):\n",
    "    '''The function generates a few convolutional network layers with 3,5 and 7\n",
    "    filter lengths on the input features.\n",
    "    Args:\n",
    "        var_input (tf.placeholder): The batch of protein features\n",
    "    Returns:\n",
    "        conv_features_output (list): List of outputs from each features\n",
    "                                     convolutional neural network layer\n",
    "     '''\n",
    "    conv_features_outputs = []\n",
    "    for size in filter_sizes:\n",
    "\n",
    "        with tf.name_scope(\"conv_freaters_%i\"%(size)):\n",
    "            # Define convolution weights and biases\n",
    "            conv_w = tf.get_variable(\"conv_features_%i\"%size, shape = [size, num_input_variables,\n",
    "               num_filter], initializer = tf.contrib.layers.xavier_initializer(seed = random_seed))\n",
    "            # Get convolution output from filters\n",
    "            conv_out_w = tf.nn.conv1d(var_input, conv_w, stride = 1,\n",
    "                        padding = \"SAME\",)\n",
    "            # Activation\n",
    "            conv_output = tf.nn.relu(conv_out_w, name = \"conv_%i_activation\"%size)\n",
    "\n",
    "            # Batch nomalization\n",
    "            conv_output = tf.layers.batch_normalization(conv_output)\n",
    "\n",
    "        conv_features_outputs.append(conv_output)\n",
    "\n",
    "    return conv_features_outputs\n",
    "\n",
    "\n",
    "def concat_layer(conv_outputs, conv_features_outputs, x_input, var_input):\n",
    "    '''Concaternate and reshape each convolution outputs by horizontally stacking\n",
    "    them together with the original inputs\n",
    "    Args:\n",
    "        conv_outputs (list): List of outputs from aa neural network layer 1\n",
    "        conv_features_outputs (list): List of output from the features nn layer 1\n",
    "        x_input (tensor): original one-hot encoded sequence\n",
    "        var_input (tensor): original features vectors\n",
    "    Returns:\n",
    "        concat_aa (tensor): concatinated tensor from amino acid CNN layer 1\n",
    "        concat_features (tensor): concatinated tensor from features CNN layer 1\n",
    "    '''\n",
    "    # Concat outputs from amino acid CNN layers\n",
    "    to_concat = [layer for layer in conv_outputs]\n",
    "    concat_outputs = tf.concat(to_concat, 2)\n",
    "\n",
    "    # Concat with original data\n",
    "    concat_aa = tf.concat([concat_outputs, x_input],2)\n",
    "\n",
    "    # Concat outputs from features CNN layers\n",
    "    to_concat_features  = [layers1 for layers1 in conv_features_outputs]\n",
    "    concat_features_outputs = tf.concat(to_concat_features, 2)\n",
    "\n",
    "    # Concat wuth original features\n",
    "    concat_features = tf.concat([concat_features_outputs, var_input],2)\n",
    "\n",
    "    return concat_aa, concat_features\n",
    "\n",
    "\n",
    "def conv_aa_layers_2(concat_aa):\n",
    "    '''The function generates the second convolutional network layers with 1,2,3\n",
    "    filter lengths for the outputs from first amino acid cnn layer.\n",
    "    Args:\n",
    "        concat_aa (tf.placeholder): The batch of one hot encoded protein sequence\n",
    "    Returns:\n",
    "        conv_output (list): List of outputs from each CNN layer\n",
    "     '''\n",
    "    conv_outputs = []\n",
    "\n",
    "    # Using 1d convolution to scan over the sequence with window of \"size\"\n",
    "    for size in filter_sizes_2:\n",
    "\n",
    "        with tf.name_scope(\"conv2_%i\"%(size)):\n",
    "            # Define convolution weights and biases\n",
    "            conv_w = tf.get_variable(\"conv2_%i\"%size, shape = [size, num_input_variables + 3*num_filter,\n",
    "               num_filter], initializer = tf.contrib.layers.xavier_initializer(seed = random_seed))\n",
    "\n",
    "            # Get convolution output from filters\n",
    "            conv_out_w = tf.nn.conv1d(concat_aa, conv_w, stride = 1,\n",
    "                        padding = \"SAME\",)\n",
    "\n",
    "            # Activation\n",
    "            conv_output = tf.nn.relu(conv_out_w, name = \"conv2_%i_activation\"%size)\n",
    "\n",
    "            # Batch normailization\n",
    "            conv_output = tf.layers.batch_normalization(conv_output)\n",
    "\n",
    "        conv_outputs.append(conv_output)\n",
    "\n",
    "    return conv_outputs\n",
    "\n",
    "\n",
    "def conv_features_layers_2(concat_features):\n",
    "    '''The function generates a few convolutional network layers with 1,2 and 3\n",
    "    filter lengths for the outputs from the first features cnn layer\n",
    "    Args:\n",
    "        concat_features (tf.placeholder): The concaternated output from the first\n",
    "                                          features CNN layer\n",
    "    Returns:\n",
    "        conv_output (list): List of outputs from each neural network\n",
    "     '''\n",
    "    conv_outputs = []\n",
    "\n",
    "    # Using 1d convolution to scan over the sequence with window of \"size\"\n",
    "    for size in filter_sizes_2:\n",
    "\n",
    "        with tf.name_scope(\"conv2_features_%i\"%(size)):\n",
    "            # Define convolution weights and biases\n",
    "            conv_w = tf.get_variable(\"conv2_features_%i\"%size, shape = [size, num_input_variables + 3*num_filter,\n",
    "               num_filter], initializer = tf.contrib.layers.xavier_initializer(seed = random_seed))\n",
    "\n",
    "            # Get convolution output from filters\n",
    "            conv_out_w = tf.nn.conv1d(concat_features, conv_w, stride = 1,\n",
    "                        padding = \"SAME\",)\n",
    "\n",
    "            # Activation\n",
    "            conv_output = tf.nn.relu(conv_out_w, name = \"conv_features_%i_activation\"%size)\n",
    "\n",
    "            conv_output = tf.layers.batch_normalization(conv_output)\n",
    "\n",
    "        conv_outputs.append(conv_output)\n",
    "\n",
    "    return conv_outputs\n",
    "\n",
    "\n",
    "def concat_layer_2(conv_outputs, conv_features_outputs, concat_aa, concat_features):\n",
    "    '''Concaternate and reshape each convolution outputs by horizontally stacking\n",
    "    them together with the first CNN outputs. The concatinate resultst form AA\n",
    "    and features together.\n",
    "    Args:\n",
    "        conv_outputs (list): List of outputs from aa neural network layer 1\n",
    "        conv_features_outputs (list): List of output from the features nn layer 1\n",
    "        concat_aa (tensor): outputs from first amino acid CNN layer\n",
    "        concat_features (tensor): outputs from first features CNN layer\n",
    "    Returns:\n",
    "        concat_input (tensor): concatinated tensor from amino acid CNN layer 1\n",
    "    '''\n",
    "    # Concat layers trained on aa\n",
    "    to_concat = [layer for layer in conv_outputs]\n",
    "    concat_outputs = tf.concat(to_concat, 2)\n",
    "    concat_outputs = tf.concat([concat_outputs,concat_aa],2)\n",
    "\n",
    "    # Concat layers trained on freatures\n",
    "    to_concat_features  = [layers1 for layers1 in conv_features_outputs]\n",
    "    concat_features_outputs = tf.concat(to_concat_features, 2)\n",
    "    concat_features_outputs = tf.concat([concat_features_outputs,concat_features], 2)\n",
    "\n",
    "    # Concat both features and aa\n",
    "    concat_input = tf.concat([concat_features_outputs,concat_outputs],2)\n",
    "\n",
    "    # First fully connected layer with 200 neurons\n",
    "    reshape_layer_1 = tf.reshape(concat_input, [batch_size * sequence_length, 2*(3*num_filter+3*num_filter+num_input_variables)])\n",
    "    layer_1 = tf.layers.dense(reshape_layer_1, hidden_neurons_1, activation=tf.nn.relu, use_bias=True)\n",
    "    concat_input =  tf.layers.batch_normalization(layer_1)\n",
    "\n",
    "    return concat_input\n",
    "\n",
    "\n",
    "def bi_nn_layer(rnn_input):\n",
    "    '''Run a bidirectional recurrent neural network on input data\n",
    "    Args:\n",
    "        rnn_input (tensor): The data to run RNN on\n",
    "    Retuns:\n",
    "        rnn_output_dropout (tensor): The output from RNN\n",
    "    '''\n",
    "    # Split the input data\n",
    "    rnn_inputs = tf.split(rnn_input, sequence_length, 0)\n",
    "\n",
    "    # bidirectional rnn\n",
    "    rnn_forward = tf.contrib.rnn.BasicLSTMCell(num_rnn_cells)\n",
    "    rnn_backward = tf.contrib.rnn.BasicLSTMCell(num_rnn_cells)\n",
    "    rnn_outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(rnn_forward,\n",
    "                        rnn_backward,rnn_inputs,dtype = tf.float32) # dtype?\n",
    "\n",
    "    # Reshape RNN output\n",
    "    rnn_outputs_reshaped = tf.reshape(rnn_outputs, [batch_size*sequence_length, 2*num_rnn_cells])\n",
    "\n",
    "    # Weight dropout for regularization\n",
    "    rnn_outputs_dropout = tf.nn.dropout(rnn_outputs_reshaped, keep_prob = 0.5)\n",
    "\n",
    "    return rnn_outputs_dropout\n",
    "\n",
    "\n",
    "def dense_layers(rnn_outputs_dropout):\n",
    "    '''3 fully connnect layers\n",
    "    Args:\n",
    "        rnn_outputs_dropout (tensor): The recurrent neural network output\n",
    "    Return:\n",
    "        dense_output (tensor): batch_size*700*9 tensor for confidence in the\n",
    "                               different categories\n",
    "    '''\n",
    "    # Fully connected layer 1\n",
    "    dense_layer = tf.layers.dense(rnn_outputs_dropout, hidden_neurons_2, activation=tf.nn.relu, use_bias=True)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    dense_layer1 = tf.layers.dense(dense_layer, hidden_neurons_3, activation=tf.nn.relu, use_bias=True)\n",
    "\n",
    "    # Last fully connected layer\n",
    "    dense_layer_2 = tf.layers.dense(dense_layer1, num_output_class,\n",
    "                use_bias=True,\n",
    "                bias_initializer = tf.random_normal_initializer(),\n",
    "                kernel_initializer = tf.random_normal_initializer())\n",
    "\n",
    "    # Reshape output\n",
    "    dense_output = tf.reshape(dense_layer_2, [batch_size,sequence_length, num_output_class])\n",
    "\n",
    "    return dense_output\n",
    "\n",
    "\n",
    "def optimization(dense_output, y_input, alpha):\n",
    "    '''Optimize model using AdamOptimizer\n",
    "    Args:\n",
    "        dense_output (tensor): The output from RNN\n",
    "        y_input (tf.placeholder): y input labels\n",
    "        alpha (float): learning rate\n",
    "    Returns:\n",
    "        optimizer (RMSPropOptimizer): optimizer used to train neural network\n",
    "        loss (float): the training loss for each epoche\n",
    "    '''\n",
    "    # Calculate loss for gradient\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dense_output, labels=y_input))\n",
    "\n",
    "    # Optimizer to minize loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate,).minimize(loss)\n",
    "    # optimizer = tf.train.RMSPropOptimizer(alpha,).minimize(loss)\n",
    "\n",
    "    return optimizer,loss\n",
    "\n",
    "\n",
    "def get_accuracy(pred,y_input):\n",
    "    '''Calcuate the prediction accuracy on the non-padded part of the sequence\n",
    "    Args:\n",
    "        pred (tensor): the predicted strucutres from the neural network model\n",
    "        y_input (tensor): the original labels of structures of input data\n",
    "    Returns:\n",
    "        accuracy (float): the accuracy of the predictions\n",
    "        seq_lens (list): the sequence lengths of each input sequence\n",
    "    '''\n",
    "    # Get actual labels\n",
    "    labels = np.argmax(y_input,2)\n",
    "\n",
    "    # Find the last aa in the squence\n",
    "    last_aa = [(l == 8).sum() for l in labels]\n",
    "    accs = []\n",
    "\n",
    "    # Calculate accuracy based on actual amino acid sequence length\n",
    "    for p,l,stop in zip(pred, labels, last_aa):\n",
    "        accs.append(np.sum(np.equal(p[:-stop],l[:-stop]))/(sequence_length-stop))\n",
    "\n",
    "    # Get lisit of sequence lengths\n",
    "    seq_lens= [700-l for l in last_aa]\n",
    "\n",
    "    return accs,seq_lens\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Reading input data...\")\n",
    "    # Get input data\n",
    "    x,f,y = read_data(file_name)\n",
    "\n",
    "    # Split training and testing data\n",
    "    x,f,y,x_test,f_test,y_test = test_batch(x,f,y,5600)\n",
    "\n",
    "    print(\"Building netowrk layers...\")\n",
    "    # First CNN layers\n",
    "    conv_aa = conv_aa_layers(x_input)\n",
    "    conv_features = conv_features_layers(var_input)\n",
    "    concat_aa, concat_features = concat_layer(conv_aa, conv_features, x_input, var_input)\n",
    "\n",
    "    # Second CNN layers\n",
    "    conv_aa_2 = conv_aa_layers_2(concat_aa)\n",
    "    conv_features_2 = conv_features_layers_2(concat_features)\n",
    "    concat_outputs_2 = concat_layer_2(conv_aa_2, conv_features_2, concat_aa, concat_features)\n",
    "\n",
    "    # Recurrent layers, dense layers, output and optimization\n",
    "    rnn_output = bi_nn_layer(concat_outputs_2)\n",
    "    dense_output = dense_layers(rnn_output)\n",
    "    optimizer,loss = optimization(dense_output, y_label, alpha)\n",
    "    #predictions = get_result(dense_output)\n",
    "\n",
    "    print(\"Start training...\")\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Training iterations\n",
    "        for i in range(epoches):\n",
    "            # Generate training and validation batches\n",
    "            x_train, f_train, y_train, x_valid, f_valid, y_valid = valid_batch(x,f,y,batch_size,valid_ratio)\n",
    "\n",
    "            # Optimize neural network\n",
    "            dense_out,cost,_ = sess.run([dense_output,loss,optimizer], feed_dict={x_input: x_train,var_input:f_train ,y_label: y_train, alpha: learning_rate})\n",
    "\n",
    "            # Loss and accuracy\n",
    "            losses.append(cost)\n",
    "            pred = tf.argmax(dense_out,2).eval()\n",
    "            acc, _ = get_accuracy(pred, y_train)\n",
    "            accs.append(sum(acc)/batch_size)\n",
    "            print(sum(acc)/batch_size)\n",
    "\n",
    "            # Print predictions every 5 epochs\n",
    "            if i % 5 == 0:\n",
    "                print(pred[0])\n",
    "                print(np.argmax(y_train,2)[0])\n",
    "\n",
    "        # Test on all data\n",
    "        train_out = sess.run(dense, feed_dict={x_input: x, var_input: f, y_input: y})\n",
    "        train_acc, train_length = get_accuracy(test_out,t_test)\n",
    "\n",
    "        # Test on training data\n",
    "        test_out = sess.run(dense, feed_dict={x_input: x_test, var_input: f_test, y_input: y_test})\n",
    "        test_acc, test_length = get_accuracy(test_out,t_test)\n",
    "\n",
    "        # Save model\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, \"./model.ckpt\")\n",
    "        print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Prediction Accuracies\n",
    "\n",
    "Since our sequences are padded with “no seq” at the end of the sequence to make it uniformly 700 amino acid long, we decided not to include these when we calculate the prediction accuracy.  For each prediction and label, we sum up how many secondary structures are predicted corrected for each amino acid that is not a “no seq”, and divide that with the actual sequence length. We take the mean of all these accuracies for each training batch.\n",
    "\n",
    "![prediction accuracy equation](Pr_023-master/prediction_accuracy_equation.png \"Prediction Accuracy Equation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Neural Network \n",
    "\n",
    "To get a sense of how well our neural network model is doing, we plotted out the softmax cross entropy loss and the training set accuracy after each epoch. We only plotted out the first 500 epoch since the learning started to plateau the and accuracies increased at a much slower rate.\n",
    "\n",
    "![epoch versus loss](Pr_023-master/epoch_loss.png \"Epoch vs. Loss\")\n",
    "\n",
    "The next plot is the cross entropy loss for the first 500 epoch.\n",
    "\n",
    "![first 500 epoch versus loss](Pr_023-master/first-500-epoch_loss.png \"First 500 Epoch vs. Loss\")\n",
    "\n",
    "After training for 1000 iterations we, tested our data with the test set that the neural network model has never seen before.\n",
    "\n",
    "![unknown test set results](Pr_023-master/unknown_test_set_results.png \"Unknown Test Set Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Correlations Within the Data\n",
    "\n",
    "After evaluating the performance of our neural network, we then tried to prove our hypothesis that properties of the protein sequence has an effect on the neural network’s ability to predict accurately. \n",
    "\n",
    "Note, all of these files can be found in the ```Pr_023/Data_Analysis/``` submission folder.\n",
    "\n",
    "<b>** Please note that we did not develop our results directly within the dataframes that we have presented. As mentioned in the data analysis secion, we ran sections of code on a groupmates computer because the results took hours to generate. **</b>\n",
    "\n",
    "### Evaluating Amino Acid Residue Representation\n",
    "\n",
    "For each protein sequence in our dataset, we calculated the percent representation of each amino acid. (The probabilities of each of the amino acids for each protein sequence will sum to 1). Using these percentages we determined that we could visualize the distribution of the appearance of each amino acid: we would do this by creating a bar graph of the average percent that each amino acid appears in each protein sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the raw data\n",
    "raw_data = np.load('cullpdb+profile_6133.npy.gz')\n",
    "\n",
    "N = len(raw_data) # number of protein training inputs\n",
    "k = 57 # number of features\n",
    "seq_length = int(len(raw_data[0]) / k) # Length of each amino acid sequence\n",
    "num_aa_labels = 21\n",
    "aa_labels = ['A', 'C', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P', 'S', 'R', 'T', 'W', 'V', 'Y', 'X']\n",
    "aa_feature_labels = ['%A', '%C', '%E', '%D', '%G', '%F', '%I', '%H', '%K', '%M', '%L', '%N', '%Q', '%P', '%S', '%R', '%T', '%W', '%V', '%Y', '%X']\n",
    "\n",
    "data = raw_data.reshape(N, seq_length, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get array of percent of each amino acid for each protein\n",
    "aa_percent = np.zeros([N, num_aa_labels])\n",
    "for i, protein in enumerate(data):\n",
    "    for j, aa in enumerate(protein[:num_aa_labels]):\n",
    "        aa_percent[i][j] = np.count_nonzero(data[i][:,j]) / seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create array that contains mean of each amino acid for all protein sequences\n",
    "mean_aa_percent = np.zeros(num_aa_labels)\n",
    "for i in range(num_aa_labels):\n",
    "    mean_aa_percent[i] = np.mean(aa_percent[:,i])\n",
    "    \n",
    "# create a pandas data fram with the percentage of amino acids for each protein\n",
    "df = pd.DataFrame(data=aa_percent, index=range(N), columns=aa_feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the Sequence Counts and add the sequence counts as a new column\n",
    "sequence_counts = [seq_length - np.count_nonzero(seq_data[:,21]) for i, seq_data in enumerate(data)]\n",
    "df['sequence_length'] = sequence_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEgCAYAAACegPWEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c25b4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot Correlation of Amino Acid Frequency in the Sequence\n",
    "ax = df[aa_feature_labels].mean(0).plot(kind='bar', title='Mean of % Amino Acid Appearance in Protein Sequences')\n",
    "ax.set_xlabel('Amino Acid Labels')\n",
    "ax.set_ylabel('% Appearance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the raw distribution of amino acid residues among all sequences, there didn't seem to be any biases towards a particular type of amino acid. This made it difficult to figure out ways we could categorize the sequences, so we abandoned this route to look for correlations between secondary structure prediction accuracy and the distribution of amino acids in a given sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Sequence Length\n",
    "\n",
    "Another thing we wanted to know is if there is any correlation between sequence length and the prediction accuracy of our model. (i.e Could it be that our model performs better when given longer sequences?) To do this we created a scatter plot out of the sequence length of each protein with regards to the percent accuracy from our model. We hypothesized that it is harder for the network to get higher accuracies on the longer sequences since there are more secondary structures to predict.\n",
    "\n",
    "We plotted the data with the following code segment taken from ```marshuang80/Protein_structure_prediction/```<a href=\"https://github.com/marshuang80/Protein_structure_prediction/blob/master/graphs_and_stats.py\"><b>```graphs_and_stats.py```</b></a>.\n",
    "\n",
    "- ```test_length``` is the length of the protein sequence that will be plotted.\n",
    "- ```test_acc``` is the prediction accuracy for that given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Scatter plot of length vs accuracy\n",
    "plt.plot(test_length,test_acc,'o')\n",
    "plt.xlabel(\"Sequence length\")\n",
    "plt.ylabel(\"% accuracy)\n",
    "z = np.ployfit(test_length,test_acc,1)\n",
    "p = np.ploy1d(z)\n",
    "plt.plot(test_length, p(test_length),'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look for statistical significance\n",
    "results = sm.OLS(test_acc,sm.add_constant(test_length)).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot we generated:\n",
    "\n",
    "![sequence length versus accuracy plot](Pr_023-master/seq-len_accuracy_plot.png \"Sequence Length vs. Accuracy Plot\")\n",
    "\n",
    "The significance table we generated:\n",
    "\n",
    "![sequence length versus accuracy significance](Pr_023-master/seq-len_accuracy_significance.png \"Sequence Length vs. Accuracy Significance\")\n",
    "\n",
    "From the graph there appears to be a negative correlation between sequence length and % accuracy, however it could be that their is a bias for shorter sequences just because there are more short sequences than longer sequences. \n",
    "\n",
    "By looking at the scatter plot, it certainly looks like there is a correlation when we add the trend line. However, when used the OLS regression result, we found out that the trendline has an R-squared value of 0.084, with 1 best the best. Thus, we failed to statistically prove a significance between the sequence length and prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Meta Secondary-Structures\n",
    "\n",
    "Finally, we wanted to know if there were any correlation between a protein’s meta secondary structure (the type of secondary structure) and the prediction accuracy of our model. To do this, we first evaluated the known secondary structures associated with a protein sequence and categorized our proteins based on if the alpha-helix structure was the most prominent structure on the protein, if the beta-sheet structure was the most prominent, and if neither of these two were prominent (in which case, we classified a protein in an “other” category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# LOADS CULLPDB+PROFILE_6133: training/validation/test set division\n",
    "# [0,5600) training\n",
    "# [5605,5877) test \n",
    "# [5877,6133) validation\n",
    "\n",
    "cullpdb_raw = np.load(\"cullpdb+profile_6133.npy.gz\")\n",
    "cullpdb = cullpdb_raw.reshape(cullpdb_raw.shape[0],700,57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# within our dataset, there are signatures for each meta secondary \n",
    "# structure at a given amino acid position according to the array below:\n",
    "secondary_label = ['L', 'B', 'E', 'G', 'I', 'H', 'S', 'T','NoSeq']\n",
    "\n",
    "# amino acids are labeled with a given secondary label if it has a value\n",
    "# of 1, if it does not have a label, it has a value of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha-helices are indicated by the G, H, and I secondary structure labels, beta-sheets are indicated by the E and B secondary structure labels, and \"other\" structures are indicated by the L, S, and T secondary structure labels. For the alpha and beta protein secondary structures, these labels are different subtypes of a given structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in this dictionary, we have categorized our secondary structure labels\n",
    "# according to what protein secondary structure it is associated with\n",
    "meta_structures = {'G':'helix', 'H':'helix', 'I':'helix',\n",
    "                  'E':'sheet', 'B':'sheet',\n",
    "                  'L':'other', 'S':'other', 'T':'other'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each protein, we looked at the secondary structure labels and tally the number of labels associated with alpha-helices, beta-sheets, and other secondary structures. Then, we calculated the percentage of the protein that is labeled as a 'helix', 'sheet', or 'other'.\n",
    "\n",
    "In order to categorize our different proteins, we placed it in the bin of the majority label: the majority percentage out of 'helix', 'sheet', or 'other'. However, if the 'helix' and 'sheet' structures are both the majority and are relatively equal, that is, within a given percent threshold of each other, then it was categorized as alpha-helix + beta-sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll start the analysis at a 5% threshold\n",
    "threshold = 5 \n",
    "\n",
    "# first, we'll make the method that will evaluate our structure counter\n",
    "def categorize_structure(structure_counter, threshold):\n",
    "    \"\"\"\n",
    "    a method which evaluates the counter of 'helix'-, 'sheet'-,\n",
    "    and 'other'-related secondary structure labels, and determines\n",
    "    if a protein should be categorized as a alpha-helix,\n",
    "    beta-sheet, alpha-helix + beta-sheet, or other \n",
    "    protein secondary structure.\n",
    "    \"\"\"\n",
    "    # add +1 to each counter to prevent 0 percentages (pseudo-counts) during calculations,\n",
    "    # and also convert the dictionary to a list with a set order (helix, sheet, then other)\n",
    "    pseudo_counts = [structure_counter[structure]+1 for structure in ['helix','sheet','other']]\n",
    "    \n",
    "    total_sum = sum(pseudo_counts)\n",
    "    pseudo_counts = [float(value)/total_sum*100 for value in pseudo_counts]\n",
    "    \n",
    "    classification = \"\"\n",
    "    if pseudo_counts.index(max(pseudo_counts)) == 0 or pseudo_counts.index(max(pseudo_counts)) == 1:\n",
    "        # checks if our maximum percent belongs to a 'helix' or 'sheet', \n",
    "        # and checks to see if they are within 5% of each other\n",
    "        if sorted(pseudo_counts[0:2])[1] - sorted(pseudo_counts[0:2])[0] <= threshold:\n",
    "            classification = \"helix+sheet\"\n",
    "        else:\n",
    "            classification = ['helix','sheet'][pseudo_counts.index(max(pseudo_counts))]\n",
    "    else: \n",
    "        # if the maximum is not a helix or a sheet, then it must be another protein secondary structure\n",
    "        classification = 'other' \n",
    "    \n",
    "    return(classification,pseudo_counts)\n",
    "    \n",
    "################################################################################\n",
    "# after writing the method, we can look at our protein sequences and categorize them!\n",
    "\n",
    "# store our categorizations for each protein within a pandas dataframe then start sorting\n",
    "df = pd.DataFrame(columns=['Secondary Structure'])\n",
    "\n",
    "# stores the percentages of the different structure labels if necessary for additional analysis\n",
    "secondary_structure_percentages = []\n",
    "\n",
    "for protein in cullpdb:\n",
    "    structure_counter = {'helix':0, 'sheet':0, 'other':0}\n",
    "    for amino_acids in protein: \n",
    "        # obtains secondary structure label for a given residue in the protein\n",
    "        label = secondary_label[np.nonzero(amino_acids[22:31] == 1)[0][0]]\n",
    "        if label == 'NoSeq':\n",
    "            # break if we've reached the end of the sequence\n",
    "            break\n",
    "        else:\n",
    "            # else, increment the counter for a given secondary structure\n",
    "            structure_counter[meta_structures[label]] += 1\n",
    "    \n",
    "    # adds our categorization to the pandas dataframe (each row is a different protein within cullpdb)\n",
    "    classification = categorize_structure(structure_counter,threshold)\n",
    "    df = df.append(\n",
    "        pd.DataFrame([classification[0]],columns=['Secondary Structure']),\n",
    "        ignore_index=True)\n",
    "    secondary_structure_percentages.append(classification[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment populated a pandas dataframe with N proteins x 1 feature, where the single feature is if we've sorted it into the alpha-helix, beta-sheet, alpha-helix + beta-sheet, or other protein secondary structure bin. \n",
    "\n",
    "We then plotted these values as a bar graph and scatterplot to see if there were any correlation between protein sequence type (those with more alpha-helicies, beta-sheets, both, or other structural labels) and protein secondary structure prediction accuracy. \n",
    "\n",
    "The code for the plots was taken from ```marshuang80/Protein_structure_prediction/```<a href=\"https://github.com/marshuang80/Protein_structure_prediction/blob/master/graphs_and_stats.py\"><b>```graphs_and_stats.py```</b></a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Bar graph for categories\n",
    "struc = df[\"Secondary Structure\"].values\n",
    "test_struc = struc[5600:]\n",
    "cat_acc = [(a,b) for a,b in zip(test_acc,test_struc)]\n",
    "\n",
    "other = np.array([a[0] for a in cat_acc if a[1] == 'other'])\n",
    "sheet = np.array([a[0] for a in cat_acc if a[1] == 'sheet'])\n",
    "helix = np.array([a[0] for a in cat_acc if a[1] == 'helix'])\n",
    "\n",
    "other_mean = other.mean()\n",
    "helix_mean = helix.mean()\n",
    "sheet_mean = sheet.mean()\n",
    "\n",
    "other_std = other.std()\n",
    "sheet_std = sheet.std()\n",
    "helix_std = helix.std()\n",
    "\n",
    "plt.bar(np.arange(3),(other_mean,sheet_mean,helix_mean),0.35,yerr=(other_std,sheet_std,helix_std)\n",
    ",color = 'rgb')\n",
    "\n",
    "plt.xticks(np.arange(3),\n",
    "          (\"other \\n %f\"%other_mean,\n",
    "            \"helix \\n %f\"%helix_mean,\n",
    "            \"sheet \\n %f\"%sheet_mean),\n",
    "            fontsize=18)\n",
    "plt.xlabel(\"Categories\",fontsize=18)\n",
    "plt.ylabel(\"Accuracy\",fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plot we generated:\n",
    "\n",
    "![meta secondary structures vs prediction accuracy bar plot](Pr_023-master/meta-secondary-structures_accuracy_barplot.png \"Meta Secondary Structures vs. Prediction Accuracy Bar Plot\")\n",
    "\n",
    "By plotting out the mean and standard deviation, we can already see that “sheet” structured proteins are 10% more accurate than “other” on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categories scatter plot\n",
    "helix_s, = plt.plot(np.arange(helix.shape[0]),helix,'bx')\n",
    "sheet_s, = plt.plot(np.arange(sheet.shape[0])*3.7,sheet,'ro')\n",
    "other_s, = plt.plot(np.arange(other.shape[0])*1.55,other,'g^')\n",
    "plt.ylabel(\"% accuracy\",fontsize=18)\n",
    "plt.legend((helix_s,other_s,sheet_s),('helix','other','sheet'),loc = 'lower right', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot we generated:\n",
    "\n",
    "![meta secondary structures vs prediction accuracy scatterplot](Pr_023-master/meta-secondary-structures_accuracy_scatterplot.png \"Meta Secondary Structures vs. Prediction Accuracy Scatterplot\")\n",
    "\n",
    "The correlation became more obvious when we did a scatter plot on these test accuracies. As shown in the plot, the sheets are mostly scattered on the top region, while “others” are found mostly on the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANOVA test\n",
    "f_val, p_val = stats.f_oneway(sheet,helix,other)\n",
    "if p_val < 0.0001:\n",
    "    print(\"Statistically different based on ANOVA variance test\")\n",
    "else:\n",
    "    print(\"Not statistically different\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we used the scipy statistic package to do an ANOVA test on our three categories, we got a p_val of 3.9e-29, thus proving that the accuracies for each structures are significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does all this data tell us?\n",
    "\n",
    "Overall, we were able to develop a tool that was able to utilize neural networks, to predict protein structure. We were able to achieve an testset accuracy of approximately 69.5% and training set accuracy of 73.45% when we train our model with 1000 iterations. This accuracy was calculated without padding because while the padding provided a higher accuracy, the padding has no relevance to a biological context and was thus removed during the calculation of the above accuracy. We further validated our results through conducting ANOVA tests to check if there is a significant difference in accuracy between sequences of different secondary structures which produced a P value of 3.97 e-29 indicating that our results are indeed significant. Therefore, we proved our initial hypothesis that there is a correlation between protein’s property and it’s prediction accuracy through a neural network.\n",
    "\n",
    "While our current steps have produced a result comparable to various protein predicting tools out there such as the <a href=\"https://arxiv.org/pdf/1702.03865.pdf\">one implemented at Google which yielded an accuracy of approximately 70%</a>, there is still room for improvement. Currently as shown in one of the figures above, there seems to be a skewing in regards to the accuracy of structures, being more accurate for structures that are sheet based rather than helical based. This can be attributed to some of the features being used such as the solvent accessibility and thus there should be consideration in adding additional features to address the skew and improve the accuracy of our tool.\n",
    "\n",
    "Furthermore, another aspect that could improve the tool is through the addition of layers within the neural network. The additional filtering would be able to provide a more accurate answer. While this approach is viable, our current approach already requires immense computational resources as shown by the amount of time required to output the data (5 - 7 hours with a 4GB Nvidia GPU). Thus, to implement this, more computational resources is required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
