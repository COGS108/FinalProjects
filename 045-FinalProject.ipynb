{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Valuation of Public Parks\n",
    "\n",
    "Authors: Chad Atalla, Alicia Chen, Nadah Feteih, Alan Chen, Joshua Van Gogh, Anjali Verma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction and Background\n",
    "----\n",
    "What value does establishing public parks add to a city and a community? Parks provide societal, commercial, and environmental benefits to the surrounding regions that are often overlooked. We will utilize hedonic pricing method to explore the value of parks and where they will provide the most value by using publicly available data from San Diego county. \n",
    " \n",
    "As we begin our research, we hypothesize that the true monetary value of public parks greatly exceeds the actual cost of land allocation and construction. This value will vary based on location and park \"features\" (house prices, health costs, etc.), but data driven estimates of this value will make it easier for cities to justify park construction costs and choose optimal locations.\n",
    "\n",
    "We have decided to collect and analyze data from these different features and categorize their impact based on police beat. Our goal is to find whether certain features strongly indicate an overall benefit in establishing public parks and if there is a strong correlation between these features and the existence of public parks in these areas.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description \n",
    "----\n",
    "\n",
    "We have decided to collect data from various sources in order to represent these features. These datasets include information about existing park locations, community recreational courses, environmental factors (storm drains), health (cost of treatment), housing (market prices), and police activity. \n",
    "\n",
    "**Dataset Name**: Park Locations (San Diego)\n",
    "* **Link to the dataset**: https://data.sandiego.gov/datasets/park-locations/\n",
    "* **Number of observations**: 258\n",
    "* **Description**: This dataset contains park locations, size, and basic content features (playgrounds, etc). The dataset is a SHP file, (common GIS format).\n",
    "\n",
    "\n",
    "**Dataset Name**: Zillow API\n",
    "* **Link to the dataset**: https://www.zillow.com/howto/api/APIOverview.htm\n",
    "* **Number of observations**: Number of observations is dependent on areas included in investigation but in San Diego County. We were able to extract around 330 observations to represent locations all around San Diego. \n",
    "* **Description**: This dataset contains “Zestimates” of the monetary values of properties in specified areas. It also contains many details about all of the properties themselves.\n",
    "\n",
    "**Dataset Name**: Health Benefits datasets\n",
    "* **Link to the dataset**: http://www.sandiegocounty.gov/content/dam/sdc/hhsa/programs/phs/documents/CHS-HealthStatusReport_Central_2012.pdf  \n",
    "  http://www.sandiegocounty.gov/content/dam/sdc/hhsa/programs/phs/documents/CHS-HealthStatusReport_NorthCentral_2012.pdf\n",
    "* **Number of observations**: Aggregate data for the regions of San Diego County\n",
    "* **Description**: The report contains community health statistics for diabetes, obesity, cancer, etc.\n",
    "\n",
    "**Dataset Name**: Environmental Benefits\n",
    "* **Links to the datasets**: http://rdw.sandag.org/Account/gisdtview?dir=StormDrain\n",
    "http://www.sciencedirect.com/science/article/pii/S1618866715300911\t \t\n",
    "* **Number of observations**: 55,187 storm drains, Economic cost per ton of CO2, Average CO2 sequestered per hectare by parks\n",
    "* **Description**: We expect parks to provide many environmental benefits including a reduction in storm runoff and CO2 sequestering. We equate the necessity of storm drains to the amount of storm runoff a region has. Parks remove an estimated cost of 23,000 dollars worth of CO2 per ha per year. We estimated that the parks in San Diego did not have as much shrubbery as the parks in the study so we rounded that value down to a conservative 1,000 dollars per acre per year.\n",
    "\n",
    "\n",
    "**Dataset Name**: Recreational Activity\n",
    "* **Link to the dataset**:  https://apm.activecommunities.com/sdparkandrec/Activity_Search?ActivityCategoryID=1&isSearch=true&applyFiltersDefaultValue=true\n",
    "* **Number of observations**: ~1,600 \n",
    "* **Description**: The community activity dataset comes from The City of San Diego Parks and Recreation Department. It details information about different general recreation and sports programs available in parks across San Diego County. The website contains lots of information about each program but for our purposes we only focused on locations and cost per individual to participate.\n",
    "\n",
    "**Dataset Name** : Safety / police activity\n",
    "* **Links** : https://data.sandiego.gov/datasets/police-beats/ - beat names and numbers\n",
    "https://data.sandiego.gov/datasets/police-calls-for-service/ - calls made to dispatch\n",
    "* **Number of observations**: ~586,000 dispatches in 2016, 128 police beats\n",
    "* **Description**: The first link provides a shapefile and csv file of the police beats of San Diego. The shapefile is used for all of our map drawings and heatmaps. The second link provides csv files for the police calls for service, we use data from 2016 and take the columns referring to dispatch instance and beat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "----\n",
    "\n",
    "Each individual source of data had to be cleaned and sanitized in a different way in order to standardize the dataset. We have each occurrence of each dataset mapped to some police beat and its corresponding beat number, as well the monetary value we have assigned to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install shapely\n",
    "!pip install pyshp\n",
    "!pip install geopy\n",
    "!pip install python-zillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapefile\n",
    "import zillow\n",
    "import random\n",
    "import json\n",
    "import sys,argparse,csv\n",
    "from geopy.geocoders import Nominatim\n",
    "from beat_from_coord import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Park Data\n",
    "The data comes in a SHP file but all we needed to know was the number and total areas of the parks within each beat. There were also a few cemeteries included in the dataset so we removed those as well since they aren’t what our hypothesis is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf = shapefile.Reader(\"city_parks/CITY_PARKS\")\n",
    "\n",
    "# get field names and data\n",
    "fields = sf.fields[1:]\n",
    "attributes = sf.records()\n",
    "\n",
    "# gather names of columns to a list\n",
    "column_names = [i[0] for i in fields]\n",
    "\n",
    "# create dataframe from the attributes\n",
    "df = pd.DataFrame(attributes)\n",
    "\n",
    "# rename column names from indices to their column name\n",
    "df =df.rename(columns={v: k for v, k in enumerate(column_names)})\n",
    "\n",
    "# add beats to dataframe\n",
    "for index, row in df.iterrows():\n",
    "    lat = float(df.iloc[index]['Y_COORD'])\n",
    "    lon = float(df.iloc[index]['X_COORD'])\n",
    "    \n",
    "    if (pd.notnull(lat) and pd.notnull(lon)):\n",
    "        beat = beat_from_coord(lat, lon)\n",
    "        df.set_value(index, 'BEAT_NUM', beat[0])\n",
    "        df.set_value(index, 'BEAT', beat[1])\n",
    "        \n",
    "# reduce columns\n",
    "df = df[['OBJECTID','COMMON_NAM','DESIG_USE','ACRES','DEDIC_PARK','BEAT','BEAT_NUM']]\n",
    "\n",
    "# remove cemeteries and failed beats\n",
    "df = df[df['DESIG_USE'] != 'Cemetery']\n",
    "df = df[df['BEAT_NUM'] != -1]\n",
    "\n",
    "# create dataframe that contains the beat_num, number of parks in the beat, and total park area.\n",
    "parks_in_beat = df['BEAT_NUM'].value_counts()\n",
    "beat_df = df.groupby('BEAT_NUM').aggregate(sum)\n",
    "beat_df['NUM_OF_PARKS'] = parks_in_beat\n",
    "beat_df['BEAT_NUM'] = beat_df.index\n",
    "\n",
    "# Get the beat area names\n",
    "sf = shapefile.Reader(\"data/beats/SDPD_BEATS\")\n",
    "shapes = sf.shapes()\n",
    "records = sf.records()\n",
    "beat_names = pd.read_csv('data/beats/beat_names.csv')\n",
    "beat_names['Neighborhood'] = beat_names['Neighborhood'].str.lower().str.strip()\n",
    "\n",
    "# Add the total beat area to beat df\n",
    "for record in records:\n",
    "    name = record[0]\n",
    "    name = record[0].lower().strip()\n",
    "    name = name[:name.find('\\x00')]\n",
    "    name = exception_lookup(name)\n",
    "    beat = beat_names[beat_names['Neighborhood'] == name]['Beat'].tolist() \n",
    "    beat = beat[0] if beat != [] else -1\n",
    "    \n",
    "    if beat != -1:\n",
    "        index = beat_df[beat_df['BEAT_NUM'] == beat].index\n",
    "        beat_df.set_value(index, 'BEAT_AREA', record[1])\n",
    "\n",
    "# Write gathered data to csv\n",
    "beat_df.to_csv('data/parks/beat_to_parks.csv')\n",
    "beat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housing Prices\n",
    "\n",
    "We are valuing houses in a region based on the average market price. In order to collect data on the market prices for houses in each police beats, we are using the Zillow API to extract the \"Zestimate\". We are randomly sampling data from the houses listed on Zillow in San Diego since we arent able to pull large amounts of data from specific areas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    key = \"X1-ZWz196b0emyd57_8p8sy\"\n",
    "\n",
    "    f = open('houseData.json', 'w')\n",
    "    minL = 16717995 \n",
    "    maxL = 16834401\n",
    "    list = []\n",
    "    for x in range(100):\n",
    "        r = random.randint(minL,maxL)\n",
    "        list.append(r)\n",
    "\n",
    "    for y in range(100):\n",
    "        api = zillow.ValuationApi()\n",
    "\n",
    "        detail_data = api.GetZEstimate(key, list[y])\n",
    "        f.write(json.dumps(detail_data.get_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are extracting the latitute and longitude and placing these houses in different police beats with the value being the Zestimate (market price) of the house. We are cleaning the data by removing any houses that dont correspond to any beats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info = []\n",
    "houses = pd.read_json('houseData.json', lines=True)\n",
    "#houses = houses.head(n=5)\n",
    "for i, row in houses.iterrows():\n",
    "    lat, long, am = row['full_address']['latitude'], row['full_address']['longitude'], row['zestimate']['amount']\n",
    "    beat, name = beat_from_coord(float(lat), float(long))\n",
    "    if (beat > -1): info.append((beat, name, am))\n",
    "    \n",
    "with open('houseVals.csv','w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerows(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Factors\n",
    "We obtained a SHP file of the locations of all the storm drains in San Diego from the Sandag website. The locations were all in the X and Y geographical coordinate system, so we imported the SHP file and DBF file into mapshaper, which we used to convert the coordinates to longitude and latitude. Using mapshaper, we were also able to convert the SHP file into a CSV file. We were then able to use our beat_from_coord function to map the longitude and latitude coordinates of each storm drain to a police beat. Using the police beat locations of each storm drain, we counted the total number of storm drains in each beat and multiplied that number by the average storm drain cost that we obtained from Homeadvisor. This allowed us to assign a monetary value to each beat area based on number of storm drains in the beat area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert the csv file of storm drain beat areas into a dataframe\n",
    "df = pd.read_csv('st_beats.csv')\n",
    "#remove data points that have an unknown beat number\n",
    "df = df[df.Beat != -1]\n",
    "#get number of storm drains in each beat area\n",
    "df_counts = df['Beat'].value_counts()\n",
    "\n",
    "#read in the csv file of beat number and beat names\n",
    "myDict = {}\n",
    "f=open('beat_names.csv', 'r')\n",
    "reader=csv.reader(f)\n",
    "#map beat number to beat name in a dictionary\n",
    "for line in reader:\n",
    "    if(line[0].isdigit()):\n",
    "        myDict[line[0]]=line[1]\n",
    "f.close()\n",
    "\n",
    "#create a list of beat names in the order that the beat numbers appear \n",
    "list_names = []\n",
    "for entry in df_counts.index:\n",
    "    list_names.append(myDict[str(entry)])\n",
    "\n",
    "#create a list of the moneatary values of each beat area\n",
    "myList = []\n",
    "for i in df_counts.values:\n",
    "    myList.append(i*(-3371))\n",
    "\n",
    "#create a new dataframe with the beat numbers corresponding to value\n",
    "new_df = pd.DataFrame({'Beat':df_counts.index, 'Count':df_counts.values, 'Value':myList, 'Name':list_names})\n",
    "print(new_df)\n",
    "#write the dataframe to a csv file\n",
    "new_df.to_csv('env_value.csv',sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Police Activity \n",
    "Since we settled on SD police beats as our regional groupings, cleaning the dispatch data was relatively simple. The dataset was made available on the San Diego data portal, and was already relatively clean: including a dispatch time and beat on each line. We removed all NaN beat entries and dropped all other columns, then grouped and counted by beat. This gave us the resulting csv with police dispatch instances per beat in the year 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOLLAR_PER_DISPATCH = 100\n",
    "\n",
    "df = pd.read_csv('data/police_dispatch/police_dispatch_2016.csv')\n",
    "bt = pd.to_numeric(df['beat'], errors='coerce')\n",
    "bt = bt.dropna()\n",
    "bt = bt.astype(int)\n",
    "btv = bt.value_counts()\n",
    "\n",
    "rbts = pd.read_csv('data/beats/beat_names.csv')\n",
    "rbtList = rbts['Beat'].astype(int).tolist()\n",
    "\n",
    "btv = btv.to_frame()\n",
    "btv['beat_n'] = btv.index\n",
    "btv['dollars'] = btv['beat']*DOLLAR_PER_DISPATCH\n",
    "btv['valid'] = btv['beat_n'].map(lambda x: x in rbtList) \n",
    "btv = btv[btv.valid == True]\n",
    "\n",
    "brief = btv[['dollars','beat_n']]\n",
    "brief = brief.reset_index()\n",
    "brief.to_csv('data/police_dispatch/police_dispatch_dollars_beat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health Care Costs\n",
    "\n",
    "The data comes from tables in PDF format with health data from the central region of San Diego. We researched to find dollar values for treating four different health conditions and based on the number of people in each region found in the tables, assigned the dollar values to the corresponding regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_health = pd.read_csv('data/health/healthDataUpdated.csv')\n",
    "location = df_health.iloc[0]['Region']\n",
    "lat = float(location[:-14])\n",
    "lon = float(location.split(\",\",1)[1])\n",
    "\n",
    "for index, row in df_health.iterrows():\n",
    "    location = df_health.iloc[index]['Region']\n",
    "    lat = float(location[:-13])\n",
    "    lon = float(location.split(\",\",1)[1])\n",
    "    #df_health[index]['Region'] = beat_from_coord(lat, lon)\n",
    "    df_health.set_value(index, 'Region', beat_from_coord(lat, lon))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Recreational Activity Data Cleaning\n",
    "The data was scraped from the website using the ParseHub app. We pulled program name, program cost per individual, and program location. After cleaning we wanted the data to map beats to a monetary value based on the dataset. We first extracted the latitudes and longitudes of each program using the address and a python library geopy. We also wanted to make sure to only use the programs that had location values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_activities = pd.read_csv('data/community/Activities.csv')\n",
    "\n",
    "#fix the one funky data entry... this is janky but it works\n",
    "df_activities['Activity_Location'][69] = \"8285 Skyline Dr\\nSan Diego, CA, US 92114\"\n",
    "\n",
    "#drop activities that don't have a location\n",
    "df_activities = df_activities.dropna()\n",
    "df_activities = df_activities.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addresses were in a format that the geopy library couldn't use. So we created a helper function to clean up the address formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_address(addr):\n",
    "    addr = addr.replace(\"\\n\", \" \", 1)\n",
    "    addr = addr[:-14]\n",
    "    return addr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pull the latitude and longitudes! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geolocator = Nominatim()\n",
    "\n",
    "latitude = []\n",
    "longitude = []\n",
    "\n",
    "for address in df_activities['Activity_Location']:\n",
    "    clean = clean_address(address)\n",
    "    geo = geolocator.geocode(str(clean))\n",
    "    if geo is None:\n",
    "        lat = np.nan\n",
    "        long = np.nan\n",
    "    else:\n",
    "        lat = geo.latitude\n",
    "        long = geo.longitude\n",
    "    latitude.append(lat)\n",
    "    longitude.append(long)\n",
    "    \n",
    "df_activities['Latitude'] = latitude\n",
    "df_activities['Longitude'] = longitude\n",
    "\n",
    "# get rid of activities that we couldn't find a longitude and latitude for\n",
    "df_activities = df_activities.dropna()\n",
    "df_activities = df_activities.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using the function beat_from_coord  we mapped each latitude and longitude to a police beat. The website did not provide how many participants were in each program so we used Fermi’s approximation to estimate the monetary value of each program. Thus, we multiplied the program cost per individual by 10 to get the monetary value each program added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beats = []\n",
    "for index, row in df_activities.iterrows():\n",
    "    #extract the beat from the lat and long\n",
    "    lat = float(df_activities.iloc[index]['Latitude'])\n",
    "    lon = float(df_activities.iloc[index]['Longitude'])\n",
    "    beats.append(beat_from_coord(lat, lon))\n",
    "    \n",
    "    price_str = df_activities.iloc[index]['Activity_price']\n",
    "    if ( price_str.find(\"Free\") == 0 ):\n",
    "        df_activities.set_value(index, 'Activity_price', 0)\n",
    "    else:\n",
    "        #Fermi's approximation\n",
    "        df_activities.set_value(index, 'Activity_price', float(price[1:-18])*10)\n",
    "    \n",
    "df_activities['Beat'] = beats\n",
    "\n",
    "#don't need these columns anymore\n",
    "df_beat_money = df_activities.drop(['Activity_url', 'Activity_Location','Activity_name', 'Clean_Addresses', 'Lattitude', 'Longitude'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization and Analysis\n",
    "----\n",
    "\n",
    "After initially cleaning each dataset, we decided to visualize the monetary value that each of these features adds to a region by graphing the correlation of each observation with amount of park space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional libraries used in visualizing data\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "#from scipy.stats import ttest_ind\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our data had major outliers that affected our predictions. This function removes them from the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make outlier remover tool\n",
    "def rem_outlier(df_in, col_name, count, how=['max', 'min']):\n",
    "    fn = max if how == 'max' else min\n",
    "    rem = df_in[df_in[col_name] == fn(df_in[col_name])]['Beat']\n",
    "    df_no = df_in[df_in.Beat != rem.values[0]]\n",
    "    for x in range(0,count-1):\n",
    "        rem = df_no[df_no[col_name] == fn(df_no[col_name])]['Beat']\n",
    "        df_no = df_no[df_no.Beat != rem.values[0]]\n",
    "    return df_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our visualization we found out that we needed to clean our data a bit more due to outliers and some incorrect data. The most notable changes include the removal of Balboa Park, as it is more a super park/cultural center than the community parks we are trying to analyze, and the reduction of Mission Bay's park area so it would only include its land mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pair number of parks and monetary value based on beat for each factor,  then find correlation\n",
    "df_parks = pd.read_csv('data/parks/beat_to_parks.csv')\n",
    "df_beats = pd.read_csv('data/beat_names_areas.csv')\n",
    "\n",
    "df_beats = df_beats.dropna()\n",
    "\n",
    "df_beats.drop('Neighborhood', axis=1, inplace=True)\n",
    "df_parks.drop('OBJECTID', axis = 1, inplace=True)\n",
    "df_parks.drop('BEAT_NUM.1', axis = 1, inplace = True)\n",
    "df_parks.drop('BEAT_AREA', axis=1, inplace=True)\n",
    "\n",
    "df_parks=df_parks.rename(columns = {'BEAT_NUM':'Beat'})\n",
    "df_parks=df_parks.rename(columns = {'ACRES':'PARK_ACRES'})\n",
    "\n",
    "# Correct for land area of mission bay\n",
    "for i in range(0, len(df_parks)):\n",
    "    if(df_parks.ix[i, 'Beat'] == 121):\n",
    "        df_parks.ix[i, 'PARK_ACRES'] = 83.16\n",
    "        \n",
    "df_parks = pd.merge(df_beats, df_parks, how='left')\n",
    "df_parks=df_parks.rename(columns={'Area':'BEAT_AREA'})\n",
    "df_parks = df_parks.fillna(value=0)\n",
    "\n",
    "# Remove outliers\n",
    "df_parks = rem_outlier(df_parks, 'PARK_ACRES', 3, how='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start analyzing any data, we can visualize where parks are in San Diego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Parks per Beat**\n",
    "<img src=\"Pr_045-master/num_parks.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acres of Park Land per Beat**\n",
    "<img src=\"Pr_045-master/acres.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health\n",
    "For health costs we first negated and then normalized all of our values for health cost by the beat size. This is because the larger the beat is the more people it is likely to have. Unfortunately, we did not have the population for beat so we had to make do with size. There is a fairly positive correlation value, showing that more parks correlate to a smaller ratio of health costs/area in a beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_combined = pd.merge(df_parks, df_health, on='Beat', how='left')\n",
    "df_combined = df_combined.fillna(value=0)\n",
    "\n",
    "# normalize value/beat_area --> $ per foot\n",
    "for i in range(0, len(df_combined)):\n",
    "    if (df_combined.ix[i, 'BEAT_AREA'] != 0):\n",
    "        df_combined.ix[i, 'Value'] =  df_combined.ix[i, 'Value']/ df_combined.ix[i, 'BEAT_AREA']\n",
    "\n",
    "# Remove outliers\n",
    "df_combined = rem_outlier(df_combined, 'Value', 4, how='min')\n",
    "        \n",
    "health_r_n, health_p =stats.pearsonr(df_combined['NUM_OF_PARKS'], df_combined['Value'])\n",
    "health_r_a, health_p =stats.pearsonr(df_combined['PARK_ACRES'], df_combined['Value'])\n",
    "\n",
    "acres = plt.scatter(df_combined['PARK_ACRES'], df_combined['Value'])\n",
    "plt.ylabel('Dollars of Health costs per acre')\n",
    "plt.xlabel('Acres of park')\n",
    "plt.title('Health Cost Scatter')\n",
    "print(\"health r, num parks:\")\n",
    "print(health_r_n)\n",
    "print(\"health r, park acres:\")\n",
    "print(health_r_a)\n",
    "#column times correlation, last column add everything up \n",
    "health_df = df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pr_045-master/health.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housing\n",
    "The housing data we had contains a few randomly sampled house values for each beat. We first grouped these values by beat and then averaged the values to get the average house value per beat. Because we got the average house value per beat we did not need to normalize the values by beat size. There is a positive correlation value, showing that more parks correlate to higher house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_housing = pd.read_csv('data/housing/finalHouseValuation.csv')\n",
    "df_housing.drop('clairemont mesa east', axis=1, inplace=True)\n",
    "df_housing=df_housing.rename(columns = {'495085':'Value'})\n",
    "df_housing=df_housing.rename(columns = {'111':'Beat'})\n",
    "\n",
    "# Average over samples per beat\n",
    "df_housing = df_housing.groupby('Beat').mean()\n",
    "df_housing['Beat'] = df_housing.index\n",
    "\n",
    "df_combined = pd.merge(df_parks, df_housing, on='Beat', how='left')\n",
    "df_combined = df_combined.fillna(value=np.mean(df_combined['Value']))\n",
    "\n",
    "housing_r_n, housing_p =stats.pearsonr(df_combined['NUM_OF_PARKS'], df_combined['Value'])\n",
    "housing_r_a, housing_p =stats.pearsonr(df_combined['PARK_ACRES'], df_combined['Value'])\n",
    "acres = plt.scatter(df_combined['PARK_ACRES'], df_combined['Value'])\n",
    "print(\"housing r, num:\")\n",
    "print(housing_r_n)\n",
    "print(\"housing r, acres:\")\n",
    "print(housing_r_a)\n",
    "\n",
    "plt.ylabel('Average housing value for beat')\n",
    "plt.xlabel('Acres of park')\n",
    "plt.title('House Value Scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pr_045-master/house.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Police Dispatch\n",
    "For police dispatches we first negated and then normalized all of our values for police dispatch cost by the beat size. This is because dispatches cost the beat money, and larger beats will generally have more police dispatches. When we ran our regression we realized that the police data we collected contained a few outliers of beasts with very high ratios of police dispatches to area. We decided to remove these outliers in our final regression. There is a slightly positive correlation value, showing that more parks correlate to a smaller ratio of police dispatches/area in a beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_police_dispatch = pd.read_csv('data/police_dispatch/police_dispatch_dollars_beat.csv')\n",
    "df_police_dispatch.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "df_police_dispatch.drop('index', axis = 1, inplace = True)\n",
    "df_police_dispatch=df_police_dispatch.rename(columns = {'dollars':'Value'})\n",
    "df_police_dispatch=df_police_dispatch.rename(columns = {'beat_n':'Beat'})\n",
    "df_police_dispatch['Value']=df_police_dispatch['Value'].map(lambda x: -x)\n",
    "\n",
    "df_combined = pd.merge(df_parks, df_police_dispatch, on='Beat', how='left')\n",
    "df_combined = df_combined.fillna(value=0)\n",
    "\n",
    "for i in range(0, len(df_combined)):\n",
    "    df_combined.ix[i, 'Value'] =  df_combined.ix[i, 'Value']/ df_combined.ix[i, 'BEAT_AREA']\n",
    "\n",
    "# Remove outliers\n",
    "df_combined = rem_outlier(df_combined, 'Value', 3, how='min')\n",
    "\n",
    "pd_r_n, pd_p =stats.pearsonr(df_combined['NUM_OF_PARKS'], df_combined['Value'])\n",
    "pd_r_a, pd_p =stats.pearsonr(df_combined['PARK_ACRES'], df_combined['Value'])\n",
    "acres = plt.scatter(df_combined['PARK_ACRES'], df_combined['Value'])\n",
    "print(\"pd r, num:\")\n",
    "print(pd_r_n)\n",
    "print(\"pd r, acres:\")\n",
    "print(pd_r_a)\n",
    "police_df = df_combined\n",
    "\n",
    "plt.ylabel('Police dispatch cost per acre')\n",
    "plt.xlabel('Acres of park')\n",
    "plt.title('Police Dispatch Scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pr_045-master/police.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreational Activities\n",
    "For recreational activities, we were not able to get data for each beat. We assumed that if we did not have a value for a beat, there was no added value to the beat. Thus we gave beats with no value a value of 0. We then ran our regression after removing outliers and saw a strong positive correlation which is what we were expecting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_community = pd.read_csv('data/community/activities_beats_money.csv')\n",
    "df_community.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "df_community=df_community.rename(columns = {'Activity_price':'Value'})\n",
    "\n",
    "beat = df_community.iloc[0]['Beat']\n",
    "beat\n",
    "\n",
    "df_community['Beat'] = df_community['Beat'].map(lambda x: x[1:4])\n",
    "df_community = df_community[df_community['Beat'] != '-1,']\n",
    "\n",
    "\n",
    "df_community['Beat'] = df_community['Beat'].astype(int)\n",
    "\n",
    "df_combined = pd.merge(df_parks, df_community, on='Beat', how='left')\n",
    "df_combined = df_combined.fillna(value=0)\n",
    "\n",
    "# Remove outliers\n",
    "df_combined = rem_outlier(df_combined, 'Value', 1, how='max')\n",
    "\n",
    "comm_r_n, comm_p =stats.pearsonr(df_combined['NUM_OF_PARKS'], df_combined['Value'])\n",
    "comm_r_a, comm_p =stats.pearsonr(df_combined['PARK_ACRES'], df_combined['Value'])\n",
    "acres = plt.scatter(df_combined['PARK_ACRES'], df_combined['Value'])\n",
    "print(\"comm r, num:\")\n",
    "print(comm_r_n)\n",
    "print(\"comm r, acres:\")\n",
    "print(comm_r_a)\n",
    "community_df = df_combined\n",
    "\n",
    "plt.ylabel('Value derived from community activies')\n",
    "plt.xlabel('Acres of park')\n",
    "plt.title('Community Activities Scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pr_045-master/community.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storm Drains\n",
    "The environment data we had revolved around storm drains, with the reasoning that parks could serve as a natural runoff for storm water and reduce the cost of building storm drains. We were able to get the locations of the storm drains and group them by beats. After counting the total number of storm drains in a beat, we normalized the data by beat area to account for the disparity in beat area sizes. We then correlated it with park area per beat and were able to see a positive correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_env = pd.read_csv('data/Environment/env_value.csv')\n",
    "\n",
    "df_env.drop('Count', axis = 1, inplace=True)\n",
    "df_env.drop('Name', axis = 1, inplace = True)\n",
    "df_env.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "df_combined = pd.merge(df_parks, df_env, on='Beat', how='left')\n",
    "df_combined = df_combined.fillna(value=0)\n",
    "\n",
    "for i in range(0, len(df_combined)):\n",
    "    df_combined.ix[i, 'Value'] =  df_combined.ix[i, 'Value']/ df_combined.ix[i, 'BEAT_AREA']\n",
    "\n",
    "\n",
    "df_combined = rem_outlier(df_combined, 'Value', 3, how='min')\n",
    "    \n",
    "env_r_n, env_p =stats.pearsonr(df_combined['NUM_OF_PARKS'], df_combined['Value'])\n",
    "env_r_a, env_p =stats.pearsonr(df_combined['PARK_ACRES'], df_combined['Value'])\n",
    "plt.scatter(df_combined['PARK_ACRES'], df_combined['Value'])\n",
    "print(\"env r, num:\")\n",
    "print(env_r_n)\n",
    "print(\"env r, acres:\")\n",
    "print(env_r_a)\n",
    "env_df = df_combined\n",
    "\n",
    "plt.ylabel('Cost of storm drains per acre')\n",
    "plt.xlabel('Acres of park')\n",
    "plt.title('Storm Drain Scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pr_045-master/env_hm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Park Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AIR_VAL_PER_ACRE = 1000\n",
    "weights = {'health': health_r_a, 'housing': housing_r_a, 'police': pd_r_a,'community': comm_r_a, 'environment': env_r_a, 'air': AIR_VAL_PER_ACRE}\n",
    "dfs = {'health': health_df, 'housing': housing_df, 'police': police_df, 'community': community_df, 'environment': env_df, 'air': df_parks['PARK_ACRES']}\n",
    "\n",
    "variables = ['health', 'housing', 'police', 'community','environment',]\n",
    "equations = ['air',]\n",
    "\n",
    "avg_beat_area = np.mean(dfs[variables[0]]['BEAT_AREA'])\n",
    "beets = dfs[variables[0]]['Beat']\n",
    "nums  = dfs[variables[0]]['NUM_OF_PARKS']\n",
    "acres = dfs[variables[0]]['PARK_ACRES']\n",
    "\n",
    "# Multiply env and police by average beat size to get dollars back\n",
    "weights['police'] *= avg_beat_area\n",
    "weights['environment'] *= avg_beat_area\n",
    "\n",
    "# Center values around zero, so that outstanding value is provided, not bias\n",
    "# ($1m house is worth only slightly more because of location)\n",
    "for data in variables:\n",
    "    dfs[data] = dfs[data]['Value'] - np.mean(dfs[data]['Value'])\n",
    "    \n",
    "first_one = variables[0]\n",
    "dfp = dfs[first_one]*weights[first_one]\n",
    "for var in variables[1:]:\n",
    "    dfp += dfs[var]*weights[var]\n",
    "for eqn in equations:\n",
    "    dfp += dfs[eqn]*weights[eqn]\n",
    "    \n",
    "# Add back in beat numbers, nums, acres\n",
    "dfp = pd.DataFrame(dfp)\n",
    "dfp['Beat'] = beets\n",
    "dfp['Num'] = nums\n",
    "dfp['Acres'] = acres\n",
    "\n",
    "# Value of parks in each beat, positive means parks add a lot of value, negative means they need more parks\n",
    "gen_heatmap(dfp, \"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map below indicates which beats are gaining a lot of total value from their parks. The parks that are red could use more parks. \n",
    "<img src=\"Pr_045-master/heat1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prep value of parks per num in beats with parks\n",
    "\n",
    "dfp = dfp.clip(lower=0)\n",
    "dfp = dfp.fillna(value=0)\n",
    "\n",
    "dfp = dfp.clip(lower=0)\n",
    "\n",
    "dfp = dfp[dfp.Num != 0]\n",
    "dfpn = dfp\n",
    "dfpn['Value'] = dfp['Value']/dfp['Num']\n",
    "gen_heatmap(dfpn, \"Greens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This map shows the average value each individual park is providing to the community for each beat. The darker green beats have parks that provide a lot of value for each park\n",
    "<img src=\"Pr_045-master/heat2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Results\n",
    "---\n",
    "\n",
    "After conducting our analysis, we found that many of the features we decided to explore did indeed have a correlation with the presence of public parks. \n",
    "\n",
    "Using Pearsons correlation, we found that the amount of money spent on health care costs and the total acreage of park space has an r value of .2818. This indicates a positive relationship of communities with lower health care costs containing larger spaces for public parks. \n",
    "\n",
    "Similarly, the correlation statistic between housing and park acreage is .1709. Although we predicted that housing prices would have a stronger correlation with park acreage, we found that our reason for error was that we werent able to scrape enough data from Zillow in order to produce a dataset that was fully representative of all police beats. \n",
    "\n",
    "The relationship between police activity and park acreage also had an r value of .2181, and the correlation of storm drains to park acreage was .1155. We were surprised to find that the strongest correlation of features was from the community centers and recreational facilities, our analysis produced an r value of .7266. \n",
    "\n",
    "After conducting the individual analysis for each feature, we generated two final heat maps: \n",
    "\n",
    "1) The first heat map shows the total value these features have had on a particular police beat and the positive impact the parks have had. The regions marked with darker, redder shades indicate that the establishment of parks in these areas would provide the greatest value and are necessary. \n",
    "\n",
    "2) The second, and final, heat map that we generated shows how much value a police beat has gained from a single public park. It is important to note that some of the regions with the greatest value gained from an individual park are centered around downtown San Diego, where parks are typically most accessible and fully utilized by the surrounding community. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---\n",
    "As our hypothesis suggested, we have found that the existence of different features within a community seems to have a positive influence on public parks. Our analysis not only shows information about current parks, but it also predicts the success and necessity of future parks. This allows city planners to consider increasing the public space allocated for parks in the specific regions we have indicated above. \n",
    "\n",
    "How can we improve our analysis of the value of public parks? \n",
    "\n",
    "By increasing the size of our datasets and collecting more observations per feature, we can increase the certainty and confidence in our results. Even though the datasets we collected each covered a different feature, our analysis may have still been limited. Our initial decision to focus on these five features to evaluate the success of public parks may have been biased and it is possible that there are many other contributing factors (such as school location and public transportation) that positively influence the success and necessity of parks. However, we have best accounted for these oversights in our initial analysis by focusing on collecting complete and adequate data for the few features that we chose and using the data to produce comprehensive analyses and results. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
